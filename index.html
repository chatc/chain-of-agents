<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Chain-of-Agents, a training-free, task-agnostic, highly interpretable framework for Long Context.">
  <meta name="keywords" content="Chain-of-Agents, Long Context, Large Language Model, Multi-agent Collaboration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chain of Agents: Large Language Models Collaborating on Long-Context Tasks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--   <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/pdf/2110.10150">
            SummN
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2110.08168">
            DYLE
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2109.04609">
            Long Dialogue Summ
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2211.05041">
            MACSum
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2311.07884">
            FairSum
          </a>
          
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Chain of Agents: Large Language Models Collaborating on Long-Context Tasks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yuszh.com/">Yusen Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/ruoxi-sun/">Ruoxi Sun</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/yanfei-chen/">Yanfei Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://tomas.pfister.fi/">Tomas Pfister</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ryanzhumich.github.io/">Rui Zhang</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.sercanarik.com/">Sercan √ñ. Arƒ±k</a><sup>*2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Penn State University,</span>
            <span class="author-block"><sup>2</sup>Google Cloud AI Research</span>
            <br>
            <span class="author-block"><sup>*</sup>Last Authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2406.02818"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.02818"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://twitter.com/YusenZhangNLP" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <!-- üíªüîó -->
                      <p style="font-size:18px">üåê</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/CoA.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Chain-of-Agents is a training-free, task-agnostic, highly interpretable framework for Long Context.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          
          <p>Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). 
          Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), 
          and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering 
          the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. 
          To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural 
          language to enable information aggregation and context reasoning across various LLMs over long-context tasks. 
          CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by 
          a manager agent who synthesizes these contributions into a coherent final output. 
          CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent 
          a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, 
          and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper Figure 1 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall Structure of Chain-of-Agent</h2>
        <img src="./static/images/coa.PNG"
             class="interpolation-image"
             alt="Overall Structure of Chain-of-Agents."/>
        <p>It consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, 
        followed by a manager agent who synthesizes these contributions into a coherent final output.</p>
      </div>
    </div>
    <!--/ Paper Figure 1 -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <h2 class="title is-3">Comparison with RAG Model</h2>
      
   <p>Overall results of CoA. CoA significantly outperforms Vanilla and RAG using various
   backbone LLMs on all datasets. CoA can also be applied in non-query tasks.</p>
    <br />
    <br />
   <img src="./static/images/main_results.PNG"
       class="interpolation-image"
       alt="Main results."/>
    <br />
    <br />

    
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
         <h3 class="title is-4">CoA Improvement is More Obvious When RAG Fails to Retrieve Gold Answer</h2>
          <p>
            Comparison on NarrativeQA. X-axis/Y-axis indicate RAG/CoA performance while each point represents a bin. 
            The number indicates the chunk index of gold answer (ratio of number of samples in bracket), 
            and the size of the point indicates the improvement of CoA over RAG. 
            Each point indicates a different retrieval quality (the number is recall @ n, lower is better).
          </p>
        </div>
      </div>
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/rag_plot.PNG"
             class="interpolation-image"
             alt="Compare with RAG."/>
          </div>
        </div>
      </div>
    </div>
   
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- RAG -->
        <h3 class="title is-4">Multi-agent Collaboration in CoA Enables Complex Reasoning over Long Context</h3>
        <div class="content has-text-justified">
          <p>
            The figure displays a sample prediction from HotpotQA. To find the correct answer, RAG retrieves text
           chunks with high semantic similarity with the query. However, conducting multi-hop reasoning is
           challenging as the critical first-hop answer often lacks semantic relevance to the query. In contrast,
           CoA operates differently: the first agent explores related topics without knowing the query‚Äôs answer,
           aiding subsequent inference. The second agent, also unaware of the answer, broadens the topic
           scope by incorporating new information. The third agent finally discovers the answer, synthesizing
           information from earlier agents and new data to complete the reasoning chain. This collaborative
           approach highlights CoA‚Äôs ability to facilitate complex reasoning across long context tasks.
          </p>
        </div>
        <img src="./static/images/rag.jfif"
               class="interpolation-image"
               alt="A case study."/>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison with Long LLMs</h2>
        <p>Comparison with long context LLMs on NarrativeQA and BookSum. CoA significantly
        outperforms Claude 3 with 200k context limits. No Trun./Trun. indicates the source text in the sample
        is less/more than 200k tokens which does not need/needs truncation for vanilla (200k) baseline.
        Average is the mean value across all samples.</p>
        <br />
        <br />
         <img src="./static/images/long_results.PNG"
             class="interpolation-image"
             alt="Long LLM results."/>
        <br />
        <br />
        <h3 class="title is-4">CoA Improvement is More Obvious When Long Context Models Meet Longer Inputs</h3>
        <div class="content has-text-justified">
          <p>
            As shown in the figure, CoA can outperform the vanilla baseline by a large margin on various source lengths.
          </p>
        </div>
        <img src="./static/images/long_full.PNG"
               class="interpolation-image"
               alt="Long LLM restuls."/>

        <!--/ Interpolating. -->

      </div>
    </div>

    <div class="columns is-centered">   
      <div class="column">
        <div class="content">
          <h3 class="title is-4">CoA Mitigates ‚ÄúLost-in-the-Middle‚Äù Phenomenon</h2>
          <p>
           To assess the ‚Äúlost-in-the-middle‚Äù effect on Vanilla and CoA models, we replicated the original study by randomly selecting 500 
            samples from their Natural Question dataset to create a QA dataset. The figure shows the performance of CoA and Full on Natural 
            Questions. CoA mitigates the lost-in-the-middle issue. X-axis is the index of document with gold answer where small number 
            indicates gold answer is closer to start.
          </p>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/lost_in_middle.PNG"
               class="interpolation-image"
               alt="CoA mitigates Lost in the middle."/>
          </div>
        </div>
      </div>
      
    </div>



<h2 class="title">Other Results and Analysis</h2>
<section class="hero is-light is-small">

  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
         <div class="item item-steve">
          <img src="./static/images/time_complexity.PNG"
               class="interpolation-image"
               alt="Time complexity."/>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/multiagent.PNG"
               class="interpolation-image"
               alt="Multiagent results."/>
        </div>  
        <div class="item item-shiba">
           <img src="./static/images/ablation_study.PNG"
               class="interpolation-image"
               alt="Ablation Study."/>
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/ensemble_results.PNG"
               class="interpolation-image"
               alt="Multi-path CoA."/>
        </div>
        
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024chain,
  author    = {Zhang, Yusen and Sun, Ruoxi and Chen, Yanfei and Pfister, Tomas and Zhang, Rui and Arƒ±k, Sercan √ñ.},
  title     = {Chain of Agents: Large Language Models Collaborating on Long-Context Tasks},
  journal   = {arXiv:2406.02818},
  year      = {2024},
}</code></pre>
  </div>
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgment</h2>
    <p>We thank Jinsung Yoon for reviewing this paper. We thank colleagues in cloud AI Research team to
provide helpful feedback for this paper.</p> 
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      @ Penn State NLP Lab 
      <a class="icon-link"
         href="https://nlp.psu.edu/">
        <i class="fas fa-home"></i>
      </a>
      <a class="icon-link" href="https://github.com/psunlpgroup" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      & Google Cloud AI Research 
      <a class="icon-link"
         href="https://research.google/teams/cloud-ai/">
        <i class="fas fa-home"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is made by Yusen Zhang, adapted from <a href="https://nerfies.github.io/">Nerfies</a>. This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
           
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
